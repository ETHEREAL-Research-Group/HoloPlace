{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import pytz\n",
    "from imitation.data.types import Trajectory\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data = pd.read_csv('data/data.csv')\n",
    "data['datetime'] = pd.to_datetime(\n",
    "    data['timestamp'], unit='ms', utc=True).dt.tz_convert(pytz.timezone('US/Mountain'))\n",
    "data.drop(['timestamp', 'scene name', 'Unnamed: 19'], axis=1, inplace=True)\n",
    "data.set_index(['datetime'], inplace=True)\n",
    "temp = ['rIndex position', 'rIndex rotation', 'rIndex velocity',\n",
    "        'rIndex angular velocity', 'lIndex position', 'lIndex rotation',\n",
    "        'lIndex velocity', 'lIndex angular velocity',\n",
    "        'gaze origin', 'gaze direction',\n",
    "        'head movement direction', 'head velocity', 'target velocity',\n",
    "        'target angular velocity', 'target position', 'target rotation']\n",
    "\n",
    "data[temp] = data[temp].applymap(literal_eval, na_action='ignore')\n",
    "dataset = pd.DataFrame()\n",
    "# dataset['Is Eye Tracking Enabled and Valid'] = data['Is Eye Tracking Enabled and Valid'].resample('0.1S').mean().interpolate('time', limit_direction='both', limit=len(data['Is Eye Tracking Enabled and Valid'].index))\n",
    "for col in temp:\n",
    "  col_data = data[col]\n",
    "  dataset[f'{col}_x'] = col_data.apply(lambda x: x if not x == x else x[0]).resample(\n",
    "      '0.1S').mean().interpolate('time', limit_direction='both', limit=len(col_data.index))\n",
    "  dataset[f'{col}_y'] = col_data.apply(lambda x: x if not x == x else x[1]).resample(\n",
    "      '0.1S').mean().interpolate('time', limit_direction='both', limit=len(col_data.index))\n",
    "  dataset[f'{col}_z'] = col_data.apply(lambda x: x if not x == x else x[2]).resample(\n",
    "      '0.1S').mean().interpolate('time', limit_direction='both', limit=len(col_data.index))\n",
    "  if col.endswith('rotation'):\n",
    "    dataset[f'{col}_w'] = col_data.apply(lambda x: x if not x == x else x[3]).resample(\n",
    "        '0.1S').mean().interpolate('time', limit_direction='both', limit=len(col_data.index))\n",
    "\n",
    "\n",
    "events = pd.read_csv('data/events.csv', usecols=['timestamp', 'event'])\n",
    "events['datetime'] = pd.to_datetime(\n",
    "    events['timestamp'], unit='ms', utc=True).dt.tz_convert(pytz.timezone('US/Mountain'))\n",
    "events.set_index(['datetime'], inplace=True)\n",
    "collision_events = events[events['event'] == 'Left IndexTip']\n",
    "target_events = events[events['event'].str.match(r'^target')]\n",
    "target_events.tail()\n",
    "del events\n",
    "\n",
    "target_found = target_events[target_events['event'] == 'target_found']\n",
    "target_lost = target_events[target_events['event'] == 'target_lost']\n",
    "\n",
    "final_res = []\n",
    "obs_concated = None\n",
    "acs_concated = None\n",
    "for found, row in target_found.iterrows():\n",
    "  lost = target_lost[target_lost.index > found].iloc[0].name\n",
    "  mask = ((dataset.index >= found) & (dataset.index <= lost))\n",
    "  masked = dataset[mask]\n",
    "  obs = masked.iloc[:, :-7].values\n",
    "  obs_concated = obs if obs_concated is None else np.concatenate([obs_concated, obs])\n",
    "  acs = masked.iloc[:, -7:].values  # - masked.iloc[:-1, -7:].values\n",
    "  acs_concated = acs if acs_concated is None else np.concatenate([acs_concated, acs])\n",
    "  # Transitions(obs, acs, None, False, None)\n",
    "  final_res.append(Trajectory(obs, acs[1:], None, True))\n",
    "  # final_res.append(TransitionsMinimal(obs, acs, np.zeros(shape=obs.shape[:1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.data.types import TransitionsMinimal\n",
    "\n",
    "demonstrations = TransitionsMinimal(obs_concated, acs_concated, np.zeros_like(obs_concated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.observation_space = spaces.Box(-1, 1, shape=(44,), dtype=np.float32)\n",
    "    self.action_space = spaces.Box(-1, 1, shape=(7,), dtype=np.float32)\n",
    "\n",
    "\n",
    "  def reset(self):\n",
    "    # We need the following line to seed self.np_random\n",
    "    # super().reset()\n",
    "\n",
    "    # Choose the agent's location uniformly at random\n",
    "    self._state = np.random.rand(44).astype('f')\n",
    "\n",
    "    return self._state\n",
    "\n",
    "  # def _get_info(self):\n",
    "  #   return None\n",
    "  \n",
    "  # def _get_obs(self):\n",
    "  #   return self._state\n",
    "\n",
    "  # def render(self):\n",
    "  #   pass\n",
    "\n",
    "  # def _render_frame(self):\n",
    "  #   pass\n",
    "\n",
    "  def step(self, action):\n",
    "    self._state[-7:] = action\n",
    "    # An episode is done iff the agent has reached the target\n",
    "\n",
    "    return self._state, 0.0, False, {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "venv = SubprocVecEnv([lambda: Monitor(CustomEnv())]*4)\n",
    "# venv = Monitor(CustomEnv())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.algorithms import bc\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "learner = PPO(env=venv, policy=MlpPolicy)\n",
    "bc_trainer = bc.BC(\n",
    "    observation_space=venv.observation_space,\n",
    "    action_space=venv.action_space,\n",
    "    demonstrations=demonstrations,\n",
    "    rng=rng,\n",
    "    policy=learner.policy,\n",
    "    batch_size=1024\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15499batch [05:12, 45.28batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 1024     |\n",
      "| bc/               |          |\n",
      "|    batch          | 15500    |\n",
      "|    ent_loss       | 0.0211   |\n",
      "|    entropy        | -21.1    |\n",
      "|    epoch          | 15500    |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 1.74e+03 |\n",
      "|    loss           | -21.2    |\n",
      "|    neglogp        | -21.3    |\n",
      "|    prob_true_act  | 2.46e+10 |\n",
      "|    samples_so_far | 15873024 |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15997batch [05:24, 46.18batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 1024     |\n",
      "| bc/               |          |\n",
      "|    batch          | 16000    |\n",
      "|    ent_loss       | 0.0211   |\n",
      "|    entropy        | -21.1    |\n",
      "|    epoch          | 16000    |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 1.78e+03 |\n",
      "|    loss           | -21      |\n",
      "|    neglogp        | -21      |\n",
      "|    prob_true_act  | 2.59e+10 |\n",
      "|    samples_so_far | 16385024 |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16495batch [05:34, 50.80batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 1024     |\n",
      "| bc/               |          |\n",
      "|    batch          | 16500    |\n",
      "|    ent_loss       | 0.0212   |\n",
      "|    entropy        | -21.2    |\n",
      "|    epoch          | 16500    |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 1.81e+03 |\n",
      "|    loss           | -21.1    |\n",
      "|    neglogp        | -21.1    |\n",
      "|    prob_true_act  | 2.75e+10 |\n",
      "|    samples_so_far | 16897024 |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16999batch [05:46, 46.63batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 1024     |\n",
      "| bc/               |          |\n",
      "|    batch          | 17000    |\n",
      "|    ent_loss       | 0.0212   |\n",
      "|    entropy        | -21.2    |\n",
      "|    epoch          | 17000    |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 1.85e+03 |\n",
      "|    loss           | -21.3    |\n",
      "|    neglogp        | -21.3    |\n",
      "|    prob_true_act  | 2.95e+10 |\n",
      "|    samples_so_far | 17409024 |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17500batch [05:57, 47.12batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 1024     |\n",
      "| bc/               |          |\n",
      "|    batch          | 17500    |\n",
      "|    ent_loss       | 0.0212   |\n",
      "|    entropy        | -21.2    |\n",
      "|    epoch          | 17500    |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 1.88e+03 |\n",
      "|    loss           | -21.3    |\n",
      "|    neglogp        | -21.3    |\n",
      "|    prob_true_act  | 3.02e+10 |\n",
      "|    samples_so_far | 17921024 |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18000batch [06:09, 45.68batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 1024     |\n",
      "| bc/               |          |\n",
      "|    batch          | 18000    |\n",
      "|    ent_loss       | 0.0213   |\n",
      "|    entropy        | -21.3    |\n",
      "|    epoch          | 18000    |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 1.91e+03 |\n",
      "|    loss           | -21.3    |\n",
      "|    neglogp        | -21.3    |\n",
      "|    prob_true_act  | 3.21e+10 |\n",
      "|    samples_so_far | 18433024 |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18500batch [06:21, 44.48batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 1024     |\n",
      "| bc/               |          |\n",
      "|    batch          | 18500    |\n",
      "|    ent_loss       | 0.0214   |\n",
      "|    entropy        | -21.4    |\n",
      "|    epoch          | 18500    |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 1.94e+03 |\n",
      "|    loss           | -21.3    |\n",
      "|    neglogp        | -21.3    |\n",
      "|    prob_true_act  | 2.83e+10 |\n",
      "|    samples_so_far | 18945024 |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18996batch [06:32, 47.24batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 1024     |\n",
      "| bc/               |          |\n",
      "|    batch          | 19000    |\n",
      "|    ent_loss       | 0.0214   |\n",
      "|    entropy        | -21.4    |\n",
      "|    epoch          | 19000    |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 1.97e+03 |\n",
      "|    loss           | -21      |\n",
      "|    neglogp        | -21      |\n",
      "|    prob_true_act  | 2.56e+10 |\n",
      "|    samples_so_far | 19457024 |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19499batch [06:45, 29.78batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 1024     |\n",
      "| bc/               |          |\n",
      "|    batch          | 19500    |\n",
      "|    ent_loss       | 0.0215   |\n",
      "|    entropy        | -21.5    |\n",
      "|    epoch          | 19500    |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 2.01e+03 |\n",
      "|    loss           | -21.4    |\n",
      "|    neglogp        | -21.4    |\n",
      "|    prob_true_act  | 3.53e+10 |\n",
      "|    samples_so_far | 19969024 |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000batch [07:01, 47.45batch/s]\n"
     ]
    }
   ],
   "source": [
    "bc_trainer.train(n_epochs=20000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.algorithms.adversarial.gail import GAIL\n",
    "\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "\n",
    "reward_net = BasicRewardNet(\n",
    "    venv.observation_space,\n",
    "    venv.action_space,\n",
    ")\n",
    "gail_trainer = GAIL(\n",
    "    demonstrations=final_res,\n",
    "    demo_batch_size=1024,\n",
    "    venv=venv,\n",
    "    gen_algo=learner,\n",
    "    reward_net=reward_net\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gail_trainer.train(16384*10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "\n",
    "\n",
    "class OnnxablePolicy(th.nn.Module):\n",
    "  def __init__(self, extractor, action_net, value_net):\n",
    "    super().__init__()\n",
    "    self.extractor = extractor\n",
    "    self.action_net = action_net\n",
    "    self.value_net = value_net\n",
    "\n",
    "  def forward(self, observation):\n",
    "    # NOTE: You may have to process (normalize) observation in the correct\n",
    "    #       way before using this. See `common.preprocessing.preprocess_obs`\n",
    "    action_hidden, value_hidden = self.extractor(observation)\n",
    "    return self.action_net(action_hidden), self.value_net(value_hidden)\n",
    "\n",
    "\n",
    "onnxable_model = OnnxablePolicy(\n",
    "    bc_trainer.policy.mlp_extractor, bc_trainer.policy.action_net, bc_trainer.policy.value_net\n",
    ")\n",
    "\n",
    "observation_size = bc_trainer.observation_space.shape\n",
    "dummy_input = th.randn(*observation_size,)\n",
    "# dummy_input.device = bc_trainer.policy.device\n",
    "th.onnx.export(\n",
    "    onnxable_model,\n",
    "    th.ones(dummy_input.shape, dtype=th.float32, device=bc_trainer.policy.device),\n",
    "    \"test.onnx\",\n",
    "    opset_version=9,\n",
    "    input_names=[\"input\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01591785708543528\n",
      "0.015917732723449995\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.01354331136069177\n",
      "0.013543346388593365\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.013888103119320084\n",
      "0.01388819170061951\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.015233893234105646\n",
      "0.0152337880161363\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.043331612495682716\n",
      "0.043331596431267945\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.04245482579786464\n",
      "0.04245493286528565\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.0310392022230322\n",
      "0.031039152642724457\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.024468696990495656\n",
      "0.02446871208182936\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.012496101587269846\n",
      "0.01249609960440825\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.015230874265723175\n",
      "0.015230946590255565\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.03996175517030087\n",
      "0.03996175568551309\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.04063416509367308\n",
      "0.04063414467015196\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.047103213881413114\n",
      "0.04710313728346026\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.0441691638573839\n",
      "0.044169174941934175\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.026068927655097798\n",
      "0.026069081080746008\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.02936982681497337\n",
      "0.029369762749838175\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.03308007650526101\n",
      "0.03308003414626361\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.03139198838561594\n",
      "0.031391998728103304\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.030934377638359736\n",
      "0.03093443805727265\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.028946576187076804\n",
      "0.028946718159129983\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.013196783932977496\n",
      "0.01319693711376171\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.019450333244079538\n",
      "0.019450319907216476\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.020037508060673354\n",
      "0.020037438517664274\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.019211085456049735\n",
      "0.019211076702255008\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.017458558953631756\n",
      "0.017458523772625755\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.014904731864132515\n",
      "0.014904669466569074\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.013131736401458849\n",
      "0.01313188259808749\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.012662091265411955\n",
      "0.012662140254716518\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.011752205898502469\n",
      "0.011752206711041902\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.010433357675024102\n",
      "0.010433300977729508\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.01032800342127459\n",
      "0.010328070227703741\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.010597121521161918\n",
      "0.010597155706976282\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.010446487939225526\n",
      "0.010446532603602494\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.011294205364446436\n",
      "0.01129412237082129\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.010924342459877015\n",
      "0.010924413791797469\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.011163458886689712\n",
      "0.011163422026699395\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.011972324743564472\n",
      "0.011972167462570276\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.012099424114255972\n",
      "0.012099270028874246\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.012695954904312306\n",
      "0.01269591975745958\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.011914990094959329\n",
      "0.011914915253064698\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.011805746057050598\n",
      "0.011805704353402701\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.011477369443329569\n",
      "0.01147744811753134\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.011307950302758378\n",
      "0.01130787554001026\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.011674982519139942\n",
      "0.011674901107924742\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.011723627496765102\n",
      "0.011723601384479273\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.012087009168445855\n",
      "0.012087004475398018\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.012526539874059953\n",
      "0.012526684805266688\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.013083528896764125\n",
      "0.013083512960294674\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.012513366818042423\n",
      "0.012513347986455146\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.013627442118131078\n",
      "0.013627533142748527\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.01401938397545247\n",
      "0.014019399739725026\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.014351312228635546\n",
      "0.014351251265079818\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.01464324514481034\n",
      "0.014643260620001116\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.014984036347602234\n",
      "0.014984109313613398\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.01412156238552852\n",
      "0.014121711963180742\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.01348950811384799\n",
      "0.013489498795229884\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.013303947285938394\n",
      "0.013303961799012777\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.013495734838145501\n",
      "0.013495853888654668\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.01442777181968703\n",
      "0.014427927351322423\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.016249021876508424\n",
      "0.016249069439844326\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.020561883950814257\n",
      "0.02056179627129538\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.020668080500239477\n",
      "0.020668085796378154\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.023373646702253576\n",
      "0.023373722947144973\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.02729685381745227\n",
      "0.02729697300958387\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.025847648308984558\n",
      "0.02584756997899438\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.007845515170639012\n",
      "0.00784566841813356\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.0042735452550492\n",
      "0.004273539333712842\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.003158637038769616\n",
      "0.003158700121188444\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.002410332153970357\n",
      "0.0024103884717759923\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.0042357608915607635\n",
      "0.004235861957906646\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.010093622131661328\n",
      "0.010093773045018302\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.009469263254322503\n",
      "0.00946921744766064\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.009086654365457848\n",
      "0.009086791587610927\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.008188967396838517\n",
      "0.008188936669447604\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.011531587441419109\n",
      "0.011531563599142149\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.011229580939631617\n",
      "0.011229684837211856\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.011927133451525511\n",
      "0.011927069968113881\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.012377510582463608\n",
      "0.012377468209999175\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.010439490483407343\n",
      "0.010439351505451418\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.008996328697234261\n",
      "0.008996392282933599\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.00674354677528089\n",
      "0.00674353258375247\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.010459808629084284\n",
      "0.010459753178077392\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.011187408686164942\n",
      "0.011187346343906808\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.012224873252832575\n",
      "0.012225007634549319\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.01297921845767605\n",
      "0.012979131645745327\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.012912550211513411\n",
      "0.0129126265246411\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.013625917911299258\n",
      "0.013625923687981403\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.013136628225823377\n",
      "0.013136768929171106\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.012069273651998287\n",
      "0.012069252586575695\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.010986905092063718\n",
      "0.010986899093159542\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.012918097868210089\n",
      "0.012918134133212243\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.021338235600753735\n",
      "0.021338297243492288\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.025815072277468123\n",
      "0.02581509118863291\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.0282742336679648\n",
      "0.02827418589227875\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.035008135757874696\n",
      "0.035008191890692604\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.04613364840336515\n",
      "0.046133726253051864\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.0749419940428008\n",
      "0.07494212566257144\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.0933282365981015\n",
      "0.09332844795180623\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.17345249670618584\n",
      "0.173452659742867\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.19989471974653902\n",
      "0.19989567012792697\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.24568635063728053\n",
      "0.24568641931550267\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.1769056527059005\n",
      "0.17690626707560683\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.07901702702698314\n",
      "0.07901712203985867\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.05166126931136618\n",
      "0.0516614543895623\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "onnx_path = \"test.onnx\"\n",
    "onnx_model = onnx.load(onnx_path)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "ort_sess = ort.InferenceSession(onnx_path)\n",
    "for idx, i in enumerate(final_res[1].obs):\n",
    "  if idx == len(final_res[1].obs) - 1:\n",
    "    continue\n",
    "  observations = i.astype(np.float32)\n",
    "  action_p, _ = learner.policy.predict(observations, deterministic=True)\n",
    "  action, value = ort_sess.run(None, {\"input\": observations})\n",
    "  print(np.linalg.norm(action - final_res[1].acts[idx]))\n",
    "  print(np.linalg.norm(action_p - final_res[1].acts[idx]))\n",
    "  print('-'*100)\n",
    "  # print(action - final_res[0].acts[idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0885a8a837be6592884a79c4ed933d20c2eac3838d44fb0a28338ac654cefeeb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
