{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import pytz\n",
    "from imitation.data.types import Trajectory\n",
    "\n",
    "data = pd.read_csv('data/data.csv')\n",
    "data['datetime'] = pd.to_datetime(\n",
    "    data['timestamp'], unit='ms', utc=True).dt.tz_convert(pytz.timezone('US/Mountain'))\n",
    "data.drop(['timestamp', 'scene name', 'Unnamed: 19'], axis=1, inplace=True)\n",
    "data.set_index(['datetime'], inplace=True)\n",
    "temp = ['gaze origin', 'target position']\n",
    "\n",
    "data[temp] = data[temp].applymap(literal_eval, na_action='ignore')\n",
    "dataset = pd.DataFrame()\n",
    "# dataset['Is Eye Tracking Enabled and Valid'] = data['Is Eye Tracking Enabled and Valid'].resample('0.1S').mean().interpolate('time', limit_direction='both', limit=len(data['Is Eye Tracking Enabled and Valid'].index))\n",
    "for col in temp:\n",
    "  col_data = data[col]\n",
    "  dataset[f'{col}_x'] = col_data.apply(lambda x: x if not x == x else x[0]).resample(\n",
    "      '0.1S').mean().interpolate('time', limit_direction='both', limit=len(col_data.index))\n",
    "  dataset[f'{col}_y'] = col_data.apply(lambda x: x if not x == x else x[1]).resample(\n",
    "      '0.1S').mean().interpolate('time', limit_direction='both', limit=len(col_data.index))\n",
    "  dataset[f'{col}_z'] = col_data.apply(lambda x: x if not x == x else x[2]).resample(\n",
    "      '0.1S').mean().interpolate('time', limit_direction='both', limit=len(col_data.index))\n",
    "  if col.endswith('rotation'):\n",
    "    dataset[f'{col}_w'] = col_data.apply(lambda x: x if not x == x else x[3]).resample(\n",
    "        '0.1S').mean().interpolate('time', limit_direction='both', limit=len(col_data.index))\n",
    "\n",
    "\n",
    "events = pd.read_csv('data/events.csv', usecols=['timestamp', 'event'])\n",
    "events['datetime'] = pd.to_datetime(\n",
    "    events['timestamp'], unit='ms', utc=True).dt.tz_convert(pytz.timezone('US/Mountain'))\n",
    "events.set_index(['datetime'], inplace=True)\n",
    "collision_events = events[events['event'] == 'Left IndexTip']\n",
    "target_events = events[events['event'].str.match(r'^target')]\n",
    "target_events.tail()\n",
    "del events\n",
    "\n",
    "target_found = target_events[target_events['event'] == 'target_found']\n",
    "target_lost = target_events[target_events['event'] == 'target_lost']\n",
    "\n",
    "final_res = []\n",
    "for found, row in target_found.iterrows():\n",
    "  lost = target_lost[target_lost.index > found].iloc[0].name\n",
    "  mask = ((dataset.index >= found) & (dataset.index <= lost))\n",
    "  masked = dataset[mask]\n",
    "  obs = masked.iloc[:, :].values\n",
    "  obs[:, :3] = obs[:, :3] - obs[:, -3:]  # gaze\n",
    "  obs = obs[:, :-3]  # removing target position\n",
    "  acs = masked.iloc[:, -3:].values\n",
    "  acs = acs[1:, :] - acs[:-1, :]\n",
    "  # for idx, item in enumerate(acs):\n",
    "  #   v = directions[idx]\n",
    "  #   v_norm = np.sqrt(sum(v**2))\n",
    "  #   if (v_norm == 0):\n",
    "  #     print('wierd')\n",
    "  #     acs[idx] = v\n",
    "  #     continue\n",
    "  #   u = item\n",
    "  #   acs[idx] = (np.dot(u, v)/v_norm**2)*v\n",
    "  final_res.append(Trajectory(obs, acs, None, True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "\n",
    "  def __init__(self, rng):\n",
    "    super().__init__()\n",
    "    self.observation_space = spaces.Box(-1, 1, shape=(3,), dtype=np.float32)\n",
    "    self.action_space = spaces.Box(-1, 1, shape=(3,), dtype=np.float32)\n",
    "    self.rng = rng\n",
    "\n",
    "  def reset(self):\n",
    "    # We need the following line to seed self.np_random\n",
    "    # super().reset()\n",
    "\n",
    "    # Choose the agent's location uniformly at random\n",
    "    self._state = (self.rng.random(3).astype('f') -0.5) * 2\n",
    "\n",
    "    return self._state\n",
    "\n",
    "  # def _get_info(self):\n",
    "  #   return None\n",
    "  \n",
    "  # def _get_obs(self):\n",
    "  #   return self._state\n",
    "\n",
    "  # def render(self):\n",
    "  #   pass\n",
    "\n",
    "  # def _render_frame(self):\n",
    "  #   pass\n",
    "\n",
    "  def step(self, action):\n",
    "    self._state[:3] -= action\n",
    "    # An episode is done iff the agent has reached the target\n",
    "\n",
    "    return self._state, 0.0, False, {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "rng = np.random.default_rng(12345)\n",
    "venv = SubprocVecEnv([lambda: Monitor(CustomEnv(rng))]*16)\n",
    "# venv = Monitor(CustomEnv())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.algorithms import bc\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "\n",
    "learner = PPO(env=venv, policy=MlpPolicy)\n",
    "bc_trainer = bc.BC(\n",
    "    observation_space=venv.observation_space,\n",
    "    action_space=venv.action_space,\n",
    "    demonstrations=final_res,\n",
    "    rng=rng,\n",
    "    policy=learner.policy,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_trainer.train(n_epochs=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.algorithms.adversarial.gail import GAIL\n",
    "from imitation.algorithms.adversarial.airl import AIRL\n",
    "\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "\n",
    "reward_net = BasicRewardNet(\n",
    "    venv.observation_space,\n",
    "    venv.action_space,\n",
    ")\n",
    "gail_trainer = AIRL(\n",
    "    demonstrations=final_res,\n",
    "    demo_batch_size=1024,\n",
    "    venv=venv,\n",
    "    gen_algo=learner,\n",
    "    reward_net=reward_net\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gail_trainer.train(32768*10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "\n",
    "\n",
    "class OnnxablePolicy(th.nn.Module):\n",
    "  def __init__(self, extractor, action_net, value_net):\n",
    "    super().__init__()\n",
    "    self.extractor = extractor\n",
    "    self.action_net = action_net\n",
    "    self.value_net = value_net\n",
    "\n",
    "  def forward(self, observation):\n",
    "    # NOTE: You may have to process (normalize) observation in the correct\n",
    "    #       way before using this. See `common.preprocessing.preprocess_obs`\n",
    "    action_hidden, value_hidden = self.extractor(observation)\n",
    "    return self.action_net(action_hidden), self.value_net(value_hidden)\n",
    "\n",
    "\n",
    "onnxable_model = OnnxablePolicy(\n",
    "    bc_trainer.policy.mlp_extractor, bc_trainer.policy.action_net, bc_trainer.policy.value_net\n",
    ")\n",
    "\n",
    "observation_size = bc_trainer.observation_space.shape\n",
    "dummy_input = th.randn(*observation_size,)\n",
    "# dummy_input.device = bc_trainer.policy.device\n",
    "th.onnx.export(\n",
    "    onnxable_model,\n",
    "    th.ones(dummy_input.shape, dtype=th.float32, device=bc_trainer.policy.device),\n",
    "    \"test.onnx\",\n",
    "    opset_version=9,\n",
    "    input_names=[\"input\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "onnx_path = \"test.onnx\"\n",
    "onnx_model = onnx.load(onnx_path)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "ort_sess = ort.InferenceSession(onnx_path)\n",
    "for idx, i in enumerate(final_res[3].obs):\n",
    "  if idx == len(final_res[3].obs) - 1:\n",
    "    continue\n",
    "  observations = i.astype(np.float32)\n",
    "  # action_p, _ = learner.policy.predict(observations, deterministic=True)\n",
    "  action, value = ort_sess.run(None, {\"input\": observations})\n",
    "  print(action)\n",
    "  print(np.linalg.norm(action - final_res[3].acts[idx]))\n",
    "  # print(np.linalg.norm(action_p - final_res[1].acts[idx]))\n",
    "  print('-'*100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0885a8a837be6592884a79c4ed933d20c2eac3838d44fb0a28338ac654cefeeb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
