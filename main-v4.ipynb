{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import pytz\n",
    "from imitation.data.types import Trajectory, TransitionsMinimal\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('data/obs_acs.csv')\n",
    "data['datetime'] = pd.to_datetime(\n",
    "    data['timestamp'], unit='ms', utc=True).dt.tz_convert(pytz.timezone('US/Mountain'))\n",
    "data.set_index(['datetime'], inplace=True)\n",
    "data.drop(['timestamp'], axis=1, inplace=True)\n",
    "data[data.columns] = data[data.columns].applymap(literal_eval, na_action='ignore')\n",
    "dataset = pd.DataFrame()\n",
    "\n",
    "for col in data.columns:\n",
    "  col_data = data[col]\n",
    "  dataset[f'{col}_x'] = col_data.apply(lambda x: x if not x == x else x[0])\n",
    "  dataset[f'{col}_y'] = col_data.apply(lambda x: x if not x == x else x[1])\n",
    "  dataset[f'{col}_z'] = col_data.apply(lambda x: x if not x == x else x[2])\n",
    "  if col.endswith('rot'):\n",
    "    dataset[f'{col}_w'] = col_data.apply(lambda x: x if not x == x else x[3])\n",
    "\n",
    "prev_idx = dataset.index[0]\n",
    "batches = []\n",
    "\n",
    "for i in dataset[dataset['act_pos_x'].isna()].index:\n",
    "  if len(batches) == 0:\n",
    "    mask = (dataset.index >= prev_idx) & (dataset.index <= i)\n",
    "  else:\n",
    "    mask = (dataset.index > prev_idx) & (dataset.index <= i)\n",
    "  prev_idx = i\n",
    "  obs = dataset[mask].values[:,:7].astype(np.float32)\n",
    "  acs = dataset[mask].values[:,7:].astype(np.float32)[:-1,:]\n",
    "  batches.append(Trajectory(obs, acs, None, False))\n",
    "\n",
    "transition_minimal = TransitionsMinimal(dataset.dropna().values[:,:7], dataset.dropna().values[:,7:], np.zeros(shape=(dataset.dropna().values.shape[0],)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "class RPMEnv(gym.Env):\n",
    "  def __init__(self, rng):\n",
    "    super().__init__()\n",
    "    self.observation_space = spaces.Box(-1, 1, shape=(7,), dtype=np.float32)\n",
    "    self.action_space = spaces.Box(-1, 1, shape=(7,), dtype=np.float32)\n",
    "    self.rng = rng\n",
    "    self.counter = 0\n",
    "\n",
    "  def get_matrix(self, flat: np.array):\n",
    "    pos = flat[:3]\n",
    "    rot = flat[3:]\n",
    "    rot_matrix = R.from_quat(rot).as_matrix()\n",
    "    res = np.zeros(shape=(4, 4), dtype=np.float32)\n",
    "    res[:3, :3] = rot_matrix\n",
    "    res[:3, 3] = pos\n",
    "    res[3, 3] = 1\n",
    "    return res\n",
    "\n",
    "  def destruct_matrix(self, matrix: np.array):\n",
    "    res = np.zeros(shape=(7,), dtype=np.float32)\n",
    "    res[:3] = matrix[:3, 3]\n",
    "    rot = R.from_matrix(matrix[:3, :3]).as_quat();\n",
    "    res[3:] = rot\n",
    "    return res\n",
    "\n",
    "  def reset(self):\n",
    "    self._state = (self.rng.random(7).astype('f') -0.5) * 2\n",
    "    self.counter = 0\n",
    "    self._info = self.get_matrix(self._state)\n",
    "\n",
    "    return self._state\n",
    "\n",
    "\n",
    "  def step(self, action):\n",
    "    act_matrix = self.get_matrix(action)\n",
    "    self.counter += 1\n",
    "    try:\n",
    "      inversed = np.linalg.inv(act_matrix)\n",
    "      self._info = np.matmul(self._info, inversed)\n",
    "      self._state = self.destruct_matrix(self._info)\n",
    "      return self._state, 0.0, self.counter > 10, {}\n",
    "    except:\n",
    "      print(\"singular matrix\")\n",
    "      return self._state, 0.0, True, {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "rng = np.random.default_rng(0)\n",
    "try:\n",
    "  venv.close()\n",
    "except:\n",
    "  pass\n",
    "if False:\n",
    "  venv = DummyVecEnv([lambda: Monitor(RPMEnv(rng))]*1)\n",
    "else:\n",
    "  venv = SubprocVecEnv([lambda: Monitor(RPMEnv(rng))]*1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.algorithms import bc\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "\n",
    "learner = PPO(env=venv, policy=MlpPolicy)\n",
    "bc_trainer = bc.BC(\n",
    "    observation_space=venv.observation_space,\n",
    "    action_space=venv.action_space,\n",
    "    demonstrations=batches[:],\n",
    "    rng=rng,\n",
    "    policy=learner.policy,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_trainer.train(n_epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_batch_len = 0\n",
    "total_len = 0\n",
    "for i in batches:\n",
    "  max_batch_len = max(max_batch_len, len(i))\n",
    "  total_len += len(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.algorithms.adversarial.gail import GAIL\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "\n",
    "reward_net = BasicRewardNet(\n",
    "    venv.observation_space,\n",
    "    venv.action_space,\n",
    ")\n",
    "gail_trainer = GAIL(\n",
    "    demonstrations=transition_minimal,\n",
    "    demo_batch_size=32,\n",
    "    venv=venv,\n",
    "    gen_algo=learner,\n",
    "    reward_net=reward_net\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "round:   0%|          | 0/1 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "EOFError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32md:\\.virtualenvs\\imitation2\\lib\\multiprocessing\\connection.py:312\u001b[0m, in \u001b[0;36mPipeConnection._recv_bytes\u001b[1;34m(self, maxsize)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m--> 312\u001b[0m     nread, err \u001b[39m=\u001b[39m ov\u001b[39m.\u001b[39;49mGetOverlappedResult(\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    313\u001b[0m     \u001b[39mif\u001b[39;00m err \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;31mBrokenPipeError\u001b[0m: [WinError 109] The pipe has been ended",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m gail_trainer\u001b[39m.\u001b[39;49mtrain(\u001b[39m2048\u001b[39;49m)\n",
      "File \u001b[1;32md:\\.virtualenvs\\imitation2\\lib\\site-packages\\imitation\\algorithms\\adversarial\\common.py:448\u001b[0m, in \u001b[0;36mAdversarialTrainer.train\u001b[1;34m(self, total_timesteps, callback)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[39massert\u001b[39;00m n_rounds \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m, (\n\u001b[0;32m    443\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mNo updates (need at least \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    444\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgen_train_timesteps\u001b[39m}\u001b[39;00m\u001b[39m timesteps, have only \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    445\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtotal_timesteps=\u001b[39m\u001b[39m{\u001b[39;00mtotal_timesteps\u001b[39m}\u001b[39;00m\u001b[39m)!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    446\u001b[0m )\n\u001b[0;32m    447\u001b[0m \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m tqdm\u001b[39m.\u001b[39mtqdm(\u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, n_rounds), desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mround\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 448\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_gen(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen_train_timesteps)\n\u001b[0;32m    449\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_disc_updates_per_round):\n\u001b[0;32m    450\u001b[0m         \u001b[39mwith\u001b[39;00m networks\u001b[39m.\u001b[39mtraining(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward_train):\n\u001b[0;32m    451\u001b[0m             \u001b[39m# switch to training mode (affects dropout, normalization)\u001b[39;00m\n",
      "File \u001b[1;32md:\\.virtualenvs\\imitation2\\lib\\site-packages\\imitation\\algorithms\\adversarial\\common.py:408\u001b[0m, in \u001b[0;36mAdversarialTrainer.train_gen\u001b[1;34m(self, total_timesteps, learn_kwargs)\u001b[0m\n\u001b[0;32m    405\u001b[0m     learn_kwargs \u001b[39m=\u001b[39m {}\n\u001b[0;32m    407\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39maccumulate_means(\u001b[39m\"\u001b[39m\u001b[39mgen\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 408\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgen_algo\u001b[39m.\u001b[39mlearn(\n\u001b[0;32m    409\u001b[0m         total_timesteps\u001b[39m=\u001b[39mtotal_timesteps,\n\u001b[0;32m    410\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    411\u001b[0m         callback\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgen_callback,\n\u001b[0;32m    412\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mlearn_kwargs,\n\u001b[0;32m    413\u001b[0m     )\n\u001b[0;32m    414\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_global_step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    416\u001b[0m gen_trajs, ep_lens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvenv_buffering\u001b[39m.\u001b[39mpop_trajectories()\n",
      "File \u001b[1;32md:\\.virtualenvs\\imitation2\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:307\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[0;32m    298\u001b[0m     \u001b[39mself\u001b[39m: SelfPPO,\n\u001b[0;32m    299\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    304\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    305\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 307\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[0;32m    308\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[0;32m    309\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    310\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    311\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[0;32m    312\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[0;32m    313\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m    314\u001b[0m     )\n",
      "File \u001b[1;32md:\\.virtualenvs\\imitation2\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:248\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    244\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[0;32m    246\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 248\u001b[0m     continue_training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, callback, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrollout_buffer, n_rollout_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_steps)\n\u001b[0;32m    250\u001b[0m     \u001b[39mif\u001b[39;00m continue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m    251\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32md:\\.virtualenvs\\imitation2\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:175\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space, spaces\u001b[39m.\u001b[39mBox):\n\u001b[0;32m    173\u001b[0m     clipped_actions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(actions, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mlow, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mhigh)\n\u001b[1;32m--> 175\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(clipped_actions)\n\u001b[0;32m    177\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mnum_envs\n\u001b[0;32m    179\u001b[0m \u001b[39m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[1;32md:\\.virtualenvs\\imitation2\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:163\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \n\u001b[0;32m    159\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 163\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[1;32md:\\.virtualenvs\\imitation2\\lib\\site-packages\\imitation\\rewards\\reward_wrapper.py:92\u001b[0m, in \u001b[0;36mRewardVecEnvWrapper.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m---> 92\u001b[0m     obs, old_rews, dones, infos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvenv\u001b[39m.\u001b[39;49mstep_wait()\n\u001b[0;32m     94\u001b[0m     \u001b[39m# The vecenvs automatically reset the underlying environments once they\u001b[39;00m\n\u001b[0;32m     95\u001b[0m     \u001b[39m# encounter a `done`, in which case the last observation corresponding to\u001b[39;00m\n\u001b[0;32m     96\u001b[0m     \u001b[39m# the `done` is dropped. We're going to pull it back out of the info dict!\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     obs_fixed \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32md:\\.virtualenvs\\imitation2\\lib\\site-packages\\imitation\\data\\wrappers.py:71\u001b[0m, in \u001b[0;36mBufferingWrapper.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_saved_acts \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     70\u001b[0m acts, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_saved_acts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_saved_acts, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m obs, rews, dones, infos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvenv\u001b[39m.\u001b[39;49mstep_wait()\n\u001b[0;32m     73\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_transitions \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs\n\u001b[0;32m     74\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32md:\\.virtualenvs\\imitation2\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\subproc_vec_env.py:121\u001b[0m, in \u001b[0;36mSubprocVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m--> 121\u001b[0m     results \u001b[39m=\u001b[39m [remote\u001b[39m.\u001b[39mrecv() \u001b[39mfor\u001b[39;00m remote \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mremotes]\n\u001b[0;32m    122\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwaiting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    123\u001b[0m     obs, rews, dones, infos \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mresults)\n",
      "File \u001b[1;32md:\\.virtualenvs\\imitation2\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\subproc_vec_env.py:121\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m--> 121\u001b[0m     results \u001b[39m=\u001b[39m [remote\u001b[39m.\u001b[39;49mrecv() \u001b[39mfor\u001b[39;00m remote \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mremotes]\n\u001b[0;32m    122\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwaiting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    123\u001b[0m     obs, rews, dones, infos \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mresults)\n",
      "File \u001b[1;32md:\\.virtualenvs\\imitation2\\lib\\multiprocessing\\connection.py:250\u001b[0m, in \u001b[0;36m_ConnectionBase.recv\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[0;32m    249\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_readable()\n\u001b[1;32m--> 250\u001b[0m buf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_recv_bytes()\n\u001b[0;32m    251\u001b[0m \u001b[39mreturn\u001b[39;00m _ForkingPickler\u001b[39m.\u001b[39mloads(buf\u001b[39m.\u001b[39mgetbuffer())\n",
      "File \u001b[1;32md:\\.virtualenvs\\imitation2\\lib\\multiprocessing\\connection.py:321\u001b[0m, in \u001b[0;36mPipeConnection._recv_bytes\u001b[1;34m(self, maxsize)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    320\u001b[0m     \u001b[39mif\u001b[39;00m e\u001b[39m.\u001b[39mwinerror \u001b[39m==\u001b[39m _winapi\u001b[39m.\u001b[39mERROR_BROKEN_PIPE:\n\u001b[1;32m--> 321\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mEOFError\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m         \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mEOFError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gail_trainer.train(2048)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "\n",
    "\n",
    "class OnnxablePolicy(th.nn.Module):\n",
    "  def __init__(self, extractor, action_net, value_net):\n",
    "    super().__init__()\n",
    "    self.extractor = extractor\n",
    "    self.action_net = action_net\n",
    "    self.value_net = value_net\n",
    "\n",
    "  def forward(self, observation):\n",
    "    # NOTE: You may have to process (normalize) observation in the correct\n",
    "    #       way before using this. See `common.preprocessing.preprocess_obs`\n",
    "    action_hidden, value_hidden = self.extractor(observation)\n",
    "    return self.action_net(action_hidden), self.value_net(value_hidden)\n",
    "\n",
    "\n",
    "onnxable_model = OnnxablePolicy(\n",
    "    bc_trainer.policy.mlp_extractor, bc_trainer.policy.action_net, bc_trainer.policy.value_net\n",
    ")\n",
    "\n",
    "observation_size = bc_trainer.observation_space.shape\n",
    "dummy_input = th.randn(*observation_size,)\n",
    "# dummy_input.device = bc_trainer.policy.device\n",
    "th.onnx.export(\n",
    "    onnxable_model,\n",
    "    th.ones(dummy_input.shape, dtype=th.float32, device=bc_trainer.policy.device),\n",
    "    \"test.onnx\",\n",
    "    opset_version=15,\n",
    "    input_names=[\"input\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "onnx_path = \"test.onnx\"\n",
    "onnx_model = onnx.load(onnx_path)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "ort_sess = ort.InferenceSession(onnx_path)\n",
    "for idx, i in enumerate(batches[0].obs):\n",
    "  if idx == len(batches[0].obs) - 1:\n",
    "    continue\n",
    "  observations = i.astype(np.float32)\n",
    "  action, value = ort_sess.run(None, {\"input\": observations})\n",
    "  print(np.linalg.norm(action - batches[0].acts[idx]))\n",
    "  print('-'*100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0885a8a837be6592884a79c4ed933d20c2eac3838d44fb0a28338ac654cefeeb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
