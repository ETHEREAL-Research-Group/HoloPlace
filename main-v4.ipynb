{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67, 7)\n",
      "(66, 7)\n",
      "(41, 7)\n",
      "(40, 7)\n",
      "(52, 7)\n",
      "(51, 7)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import pytz\n",
    "from imitation.data.types import Trajectory\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('data/obs_acs.csv')\n",
    "data['datetime'] = pd.to_datetime(\n",
    "    data['timestamp'], unit='ms', utc=True).dt.tz_convert(pytz.timezone('US/Mountain'))\n",
    "data.set_index(['datetime'], inplace=True)\n",
    "data.drop(['timestamp'], axis=1, inplace=True)\n",
    "data[data.columns] = data[data.columns].applymap(literal_eval, na_action='ignore')\n",
    "dataset = pd.DataFrame()\n",
    "\n",
    "for col in data.columns:\n",
    "  col_data = data[col]\n",
    "  dataset[f'{col}_x'] = col_data.apply(lambda x: x if not x == x else x[0])\n",
    "  dataset[f'{col}_y'] = col_data.apply(lambda x: x if not x == x else x[1])\n",
    "  dataset[f'{col}_z'] = col_data.apply(lambda x: x if not x == x else x[2])\n",
    "  if col.endswith('rot'):\n",
    "    dataset[f'{col}_w'] = col_data.apply(lambda x: x if not x == x else x[3])\n",
    "\n",
    "prev_idx = dataset.index[0]\n",
    "batches = []\n",
    "for i in dataset[dataset['act_pos_x'].isna()].index:\n",
    "  if len(batches) == 0:\n",
    "    mask = (dataset.index >= prev_idx) & (dataset.index <= i)\n",
    "  else:\n",
    "    mask = (dataset.index > prev_idx) & (dataset.index <= i)\n",
    "  prev_idx = i\n",
    "  obs = dataset[mask].values[:,:7].astype(np.float32)\n",
    "  acs = dataset[mask].values[:,7:].astype(np.float32)[:-1,:]\n",
    "  print(obs.shape)\n",
    "  print(acs.shape)\n",
    "  batches.append(Trajectory(obs, acs, None, False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "\n",
    "  def __init__(self, rng):\n",
    "    super().__init__()\n",
    "    self.observation_space = spaces.Box(-1, 1, shape=(7,), dtype=np.float32)\n",
    "    self.action_space = spaces.Box(-1, 1, shape=(7,), dtype=np.float32)\n",
    "    self.rng = rng\n",
    "\n",
    "  def get_matrix(self, flat: np.array):\n",
    "    pos = flat[:3]\n",
    "    rot = flat[3:]\n",
    "    rot_matrix = R.from_quat(rot).as_matrix()\n",
    "    res = np.zeros(shape=(4, 4), dtype=np.float32)\n",
    "    res[:3, :3] = rot_matrix\n",
    "    res[:3, 3] = pos\n",
    "    res[3, 3] = 1\n",
    "    return res\n",
    "\n",
    "  def destruct_matrix(self, matrix: np.array):\n",
    "    res = np.zeros(shape=(7,), dtype=np.float32)\n",
    "    res[:3] = matrix[:3, 3]\n",
    "    rot = R.from_matrix(matrix[:3, :3]).as_quat();\n",
    "    res[3:] = rot\n",
    "    return res\n",
    "\n",
    "  def reset(self):\n",
    "    self._state = (self.rng.random(7).astype('f') -0.5) * 2\n",
    "\n",
    "    self._info = self.get_matrix(self._state)\n",
    "\n",
    "    return self._state\n",
    "\n",
    "\n",
    "  def step(self, action):\n",
    "    act_matrix = self.get_matrix(action)\n",
    "    try:\n",
    "      inversed = np.linalg.inv(act_matrix)\n",
    "      self._info = np.matmul(self._info, inversed)\n",
    "      self._state = self.destruct_matrix(self._info)\n",
    "      return self._state, 0.0, False, {}\n",
    "    except:\n",
    "      print(\"singular matrix\")\n",
    "      return self._state, 0.0, True, {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "rng = np.random.default_rng(12345)\n",
    "venv = SubprocVecEnv([lambda: Monitor(CustomEnv(rng))]*1)\n",
    "# venv = Monitor(CustomEnv())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.algorithms import bc\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "\n",
    "learner = PPO(env=venv, policy=MlpPolicy)\n",
    "bc_trainer = bc.BC(\n",
    "    observation_space=venv.observation_space,\n",
    "    action_space=venv.action_space,\n",
    "    demonstrations=batches[:],\n",
    "    rng=rng,\n",
    "    policy=learner.policy,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0batch [00:00, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 0        |\n",
      "|    ent_loss       | -0.00711 |\n",
      "|    entropy        | 7.11     |\n",
      "|    epoch          | 0        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 147      |\n",
      "|    loss           | 3.6      |\n",
      "|    neglogp        | 3.61     |\n",
      "|    prob_true_act  | 0.0271   |\n",
      "|    samples_so_far | 32       |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "494batch [00:01, 256.34batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 500      |\n",
      "|    ent_loss       | -0.0036  |\n",
      "|    entropy        | 3.6      |\n",
      "|    epoch          | 125      |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 150      |\n",
      "|    loss           | 0.106    |\n",
      "|    neglogp        | 0.109    |\n",
      "|    prob_true_act  | 0.897    |\n",
      "|    samples_so_far | 16032    |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "978batch [00:03, 287.72batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 1000      |\n",
      "|    ent_loss       | -0.000106 |\n",
      "|    entropy        | 0.106     |\n",
      "|    epoch          | 250       |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 158       |\n",
      "|    loss           | -3.38     |\n",
      "|    neglogp        | -3.38     |\n",
      "|    prob_true_act  | 29.3      |\n",
      "|    samples_so_far | 32032     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1481batch [00:05, 296.99batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 1500     |\n",
      "|    ent_loss       | 0.00339  |\n",
      "|    entropy        | -3.39    |\n",
      "|    epoch          | 375      |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 165      |\n",
      "|    loss           | -6.86    |\n",
      "|    neglogp        | -6.86    |\n",
      "|    prob_true_act  | 958      |\n",
      "|    samples_so_far | 48032    |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1985batch [00:07, 235.86batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 2000     |\n",
      "|    ent_loss       | 0.00686  |\n",
      "|    entropy        | -6.86    |\n",
      "|    epoch          | 500      |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 178      |\n",
      "|    loss           | -10.2    |\n",
      "|    neglogp        | -10.2    |\n",
      "|    prob_true_act  | 2.96e+04 |\n",
      "|    samples_so_far | 64032    |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2479batch [00:09, 308.27batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 2500     |\n",
      "|    ent_loss       | 0.0103   |\n",
      "|    entropy        | -10.3    |\n",
      "|    epoch          | 625      |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 193      |\n",
      "|    loss           | -13.7    |\n",
      "|    neglogp        | -13.7    |\n",
      "|    prob_true_act  | 9.42e+05 |\n",
      "|    samples_so_far | 80032    |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2972batch [00:10, 276.20batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 3000     |\n",
      "|    ent_loss       | 0.0137   |\n",
      "|    entropy        | -13.7    |\n",
      "|    epoch          | 750      |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 206      |\n",
      "|    loss           | -17      |\n",
      "|    neglogp        | -17.1    |\n",
      "|    prob_true_act  | 2.64e+07 |\n",
      "|    samples_so_far | 96032    |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3494batch [00:12, 305.85batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 3500     |\n",
      "|    ent_loss       | 0.0169   |\n",
      "|    entropy        | -16.9    |\n",
      "|    epoch          | 875      |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 220      |\n",
      "|    loss           | -19.2    |\n",
      "|    neglogp        | -19.2    |\n",
      "|    prob_true_act  | 5.65e+08 |\n",
      "|    samples_so_far | 112032   |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000batch [00:14, 276.80batch/s]\n"
     ]
    }
   ],
   "source": [
    "bc_trainer.train(n_epochs=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "\n",
    "\n",
    "class OnnxablePolicy(th.nn.Module):\n",
    "  def __init__(self, extractor, action_net, value_net):\n",
    "    super().__init__()\n",
    "    self.extractor = extractor\n",
    "    self.action_net = action_net\n",
    "    self.value_net = value_net\n",
    "\n",
    "  def forward(self, observation):\n",
    "    # NOTE: You may have to process (normalize) observation in the correct\n",
    "    #       way before using this. See `common.preprocessing.preprocess_obs`\n",
    "    action_hidden, value_hidden = self.extractor(observation)\n",
    "    return self.action_net(action_hidden), self.value_net(value_hidden)\n",
    "\n",
    "\n",
    "onnxable_model = OnnxablePolicy(\n",
    "    bc_trainer.policy.mlp_extractor, bc_trainer.policy.action_net, bc_trainer.policy.value_net\n",
    ")\n",
    "\n",
    "observation_size = bc_trainer.observation_space.shape\n",
    "dummy_input = th.randn(*observation_size,)\n",
    "# dummy_input.device = bc_trainer.policy.device\n",
    "th.onnx.export(\n",
    "    onnxable_model,\n",
    "    th.ones(dummy_input.shape, dtype=th.float32, device=bc_trainer.policy.device),\n",
    "    \"test.onnx\",\n",
    "    opset_version=9,\n",
    "    input_names=[\"input\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03027010770383833\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.012832886637383979\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.02168983446296189\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.02759699900934911\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.02558587882249591\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.025872564846956318\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.017833979950414646\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.011649364508439349\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.014342902146064125\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.016298645243663295\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.02113869790231667\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.01979694194117676\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.026618792675196477\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.01916286552442985\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.02038295377357262\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.020996394249616548\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.013407530254391656\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.018677023237978974\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.027195972316558813\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.02636398535221117\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.03467313988450235\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.10735929337682631\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.031039890689015553\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.09982144223458453\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.06841506599698664\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.04157163269769623\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.03913780558737228\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.026662478757671363\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.047854744471650126\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.06251180661978492\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.17411265094304254\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.17801248965945993\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.047514234578925035\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.0699152471648802\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.04360072479997144\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.02821644916091168\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.03634993845374154\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.04755197095877712\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.034340885972416954\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.018168417596915946\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.014765099621182994\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.016766248588063637\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.028071505987512676\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.016396715536517916\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.011795699264419243\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.014488316437778627\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.012581224348450586\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.014918204194599176\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.014504565409775603\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.012247433444042964\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.009897245733902656\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.009856301962650704\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.010441146680314773\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.017153863533801727\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.00935236431350739\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.007692391242040512\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.011268227189991813\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.010800383764435862\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.011825899900651828\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.01407101233671911\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.016901702007700484\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.0139297571842659\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.016976056513527422\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.023135957661187822\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.017722487979123333\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.017898655906467192\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.017001675244321838\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.015684932607145655\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.01300222086872069\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.015300566432339726\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.02847583347738513\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.028355728112383075\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.0262869065202849\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.02558357921136169\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.029679141624378562\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.01379880463223066\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.01920333306364901\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.02824101272636467\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.05616397073068965\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.05752121920598301\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.05975737274957183\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.026480170066890945\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.04089368089444055\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.06105100760240089\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.06207637697963793\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.08816266900407077\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.0912041951566635\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.09415754810604711\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.07961764537262564\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.05410729979882472\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.017364863300558107\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.05606743835344908\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.15423441782926745\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.2218585584521091\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.06135713174823118\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.12360687823883673\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "onnx_path = \"test.onnx\"\n",
    "onnx_model = onnx.load(onnx_path)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "ort_sess = ort.InferenceSession(onnx_path)\n",
    "for idx, i in enumerate(batches[0].obs):\n",
    "  if idx == len(batches[0].obs) - 1:\n",
    "    continue\n",
    "  observations = i.astype(np.float32)\n",
    "  # action_p, _ = learner.policy.predict(observations, deterministic=True)\n",
    "  action, value = ort_sess.run(None, {\"input\": observations})\n",
    "  print(np.linalg.norm(action - batches[0].acts[idx]))\n",
    "  # print(np.linalg.norm(action_p - final_res[1].acts[idx]))\n",
    "  print('-'*100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0885a8a837be6592884a79c4ed933d20c2eac3838d44fb0a28338ac654cefeeb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
